---
title: "Análisis de datos Bayesiano"
date: "23 de octubre de 2018"
output:
    html_document:
        css: ../codigo-estilos/cajas.css
        theme: spacelab
---

<!-- runtime: shiny -->

```{r options, echo = FALSE, message=FALSE, error=TRUE, warning=FALSE}
knitr::opts_chunk$set(
    comment = "#>",
    collapse = TRUE, 
    cache=TRUE
  )
comma <- function(x) format(x, digits = 2, big.mark = ",")
options(digits=3)

library(tidyverse)
library(gridExtra)
theme_set(theme_minimal())
```


Para esta sección seguiremos el libro [Doing Bayesian Data Analysis](https://sites.google.com/site/doingbayesiandataanalysis/) de John K. Kruschke.

Hasta ahora hemos estudiado métodos estadísticos frecuentistas (o clásicos). El 
punto de vista frecuentista se basa en los siguientes puntos (Wasserman, 2005):

1. La probabilidad se refiere a un límite de frecuencias relativas, las 
probabilidades son propiedades objetivas en el mundo real.

2. En un modelo, los parámetros son constantes fijas (desconocidas). Como 
consecuencia de que los parámetros son constantes no se pueden realizar 
afirmaciones probabilísticas en relación a éstos.

3. Los procedimientos estadísticos deben diseñarse con el objetivo de tener 
propiedades frecuentistas bien definidas. Por ejemplo, un intervalo de confianza 
del 95% debe contener el verdadero valor del parámetro con frecuencia límite
del 95%.

Por su parte, el paradigma Bayesiano se basa en los siguientes postulados:

1. La probabilidad describe grados de creencia, no frecuencias limite. Como 
tal, uno puede hacer afirmaciones probabilísticas acerca de muchas cosas y no 
solo acerca de datos sujetos a variabilidad aleatoria. Por ejemplo, puedo decir: "La 
probabilidad de que Einstein tomara una copa de te el 1ro. de agosto de 1948 es del 35%"; esta proposición no hace referencia a ninguna frecuencia relativa sino que refleja
la certeza que yo tengo de que sea verdadera.

2. Podemos hacer afirmaciones probabilísticas de parámetros.

3. Podemos hacer inferencia estadística de un parámetro $\theta$ por medio de 
distribuciones de probabilidad. Las inferencias como estimaciones puntuales y 
estimaciones por intervalos se pueden extraer de dicha distribución.

Kruschke describe los puntos de arriba como dos ideas fundamentales del 
análisis bayesiano:

* La inferencia bayesiana es la reubicación de creencias a lo largo de posibilidades.

* Las posibilidades son valores de los parámetros en modelos descriptivos.

## Probabilidad subjetiva
¿Qué tanta certeza tienes de que una moneda acuñada por la casa de moneda 
mexicana es justa? Si, en cambio, consideramos una moneda antigua y asimétrica, 
¿creemos que es justa? En estos escenarios no estamos considerando la verdadera
probabilidad, inherente a la moneda. Lo que queremos medir es el grado en que 
creemos que cada probabilidad puede ocurrir, en lugar de una serie de experimentos llevados a cabo con las monedas.

Para especificar nuestras creencias debemos medir que tan verosímil pensamos
que es cada posible resultado. Describir con presición nuestras creencias puede
ser una tarea difícil, por lo que exploraremos como _calibrar_ las creencias
subjetivas.


#### Calibración

Considere una pregunta sencilla que puede afectar a un viajero: ¿Qué tanto 
crees que habrá una tormenta que ocasionará el cierre de la autopista
México-Acapulco en el puente del 20 de noviembre? Como respuesta debes dar
un número entre 0 y 1 que refleje tus creencias. Una manera de seleccionar 
dicho número es **calibrar** las creencias en relación a otros eventos cuyas 
probabilidades sean claras.

Como evento de comparación considere un experimento donde hay canicas en una
urna: 5 rojas y 5 blancas. Seleccionamos una canica al azar. Usaremos esta urna
como comparación para calibrar nuestra creencia acerca de la tormenta en la autopista. Ahora, considere
el siguiente par de apuestas de las cuales puede elegir sólamente una:

A. Obtiene $1000 si hay una tormenta que ocasiona el cierre de la autopista
el próximo 20 de noviembre.

B. Obtiene $1000 si seleccionas una canica roja de la urna que contiene 
5 canicas rojas y 5 blancas.

Si prefiere la apuesta B quiere decir que considera que la probabilidad de 
tormenta es menor a 0.5 (50\%), por lo que al menos sabe que su creencia subjetiva de 
una la probabilidad de tormenta es menor a 0.5. Podemos continuar con el proceso
para tener una mejor estimación de la creencia subjetiva.

A. Obtiene $1000 si hay una tormenta que ocasiona el cierre de la autopista
el próximo 20 de noviembre.

C. Obtiene $1000 si selecciona una canica roja de la urna que contiene 
1 canica roja y 9 blancas.

Si ahora selecciona la apuesta A, esto querría decir que considera que la 
probabilidad de que ocurra una tormenta es mayor a 0.1 (10\%). Si consideramos ambas 
comparaciones tenemos que su probabilidad subjetiva se ubica entre 0.1 y 0.5.

Por tanto, el proceso de calibración nos permite cuantificar nuestras creencias subjetivas. Sin embargo, cuando hay muchos posibles resultados de un evento, es practicamente 
imposible calibrar las creencias subjetivas para cada resultado; en su lugar,
podemos usar una función matemática.

**Ejercicio:** ¿Cuántos analfabetas dirías que había en México en 2015?
Da un intervalo del 90% de confianza para esta cantidad.

Más de calibración:

* Prueba de calibración de [Messy Matters](http://messymatters.com/calibration/).

* Más pruebas en [An Educated Guess](http://sethrylan.org/bayesian/index.html).

#### Descripción matemática de creencias subjetivas

**Ejemplo:** podemos pensar que una mujer mexicana promedio mide 156 cm pero 
estar abierto a la posibilidad de que el promedio sea un poco mayor o menor. 
En este caso podríamos describir nuestras creencias a través de una curva con forma
de campana y centrada en 156. 

No olvidemos que estamos describiendo 
probabilidades, subjetivas o no deben cumplir los axiomas de probabilidad. Es
por esto que la función matemática debe conformar una distribuión de probabilidad.

En el caso de parámetros $\theta$, si $p(\theta)$ representa el grado de nuestra creencia en los valores de
$\theta$, entonces la media de $p(\theta)$ se puede pensar como un valor de
$\theta$ que representa nuestra creencia típica o central. Por su parte, 
la varianza de $\theta$, que mide que tan dispersa esta la distribución, se 
puede pensar como la incertidumbre entre los posibles valores.


## Regla de Bayes

Thomas Bayes (1702-1761) fue un matemático y ministro de la iglesia 
presbiteriana, en 1764 se publicó su famoso teorema. 

Una aplicación crucial de la regla de Bayes es determinar la probabilidad de un 
modelo dado un conjunto de datos. Lo que el modelo determina es la probabilidad 
de los datos condicional a valores particulares de los parámetros y a la 
estructura del modelo. 

Por su parte, podemos utilizar la regla de Bayes para ir de la 
probabilidad de los datos, dado el modelo, a la probabilidad del modelo, dados 
los datos. 

**Ejemplo: Lanzamientos de monedas.** Comencemos recordando la regla de Bayes usando dos variables aleatorias 
discretas. Lanzamos una moneda 3 veces, sea $X$ la variable aleatoria 
correspondiente al número de águilas observadas y $Y$ registra el número de 
cambios entre águilas y soles.

1. Escribe la distribución conjunta de las variables, y las distribuciones
marginales.

2. Considera la probabilidad de observar un cambio condicional a que observamos 
un águila y compara con la probabilidad de observar un águila condicional a 
que observamos un cambio.

Para entender probabilidad condicional podemos pensar en restringir nuestra 
atención a una única fila o columna de 
la tabla. Supongamos que alguien lanza una moneda 3 veces y nos informa que 
la secuencia contiene  exactamente un cambio. Dada esta información podemos
restringir nuestra atención a la fila correspondiente a un solo cambio. Sabemos
que ocurrió uno de los eventos de esa fila. Las probabilidades relativas
de los eventos de esa fila no han cambiado pero sabemos que la probabilidad 
total debe sumar uno, por lo que simplemente normalizamos dividiendo entre
$p(C=1)$. 

Por otro lado, cuando no sabemos nada acerca del número de 
cambios, todo lo que sabemos del número de águilas está contenido en la 
distribución marginal de $X$.


### Regla de Bayes en modelos y datos

Una de las aplicaciones más importantes de la regla de Bayes es cuando las 
variables fila $X$ y columna $Y$ son datos y parámetros del modelo, respectivamente.

Un modelo especifica la probabilidad de valores particulares de los datos dado la estructura
del modelo y valores de los parámetros. 

**Ejemplo:** En un modelo de lanzamientos de monedas tenemos $p(x = A|\theta)=\theta$ y $p(x = S|\theta)= 1- \theta$. 

De manera general, el modelo especifica:
$$p(datos|\text{valores de parámetros y estructura del modelo})$$

y usamos la regla de Bayes para convertir la expresión anterior a lo que 
nos interesa de verdad, que es, que tanta certidumbre tenemos del modelo
condicional a los datos:

$$p(\text{valores de parámetros y estructura del modelo} | datos)$$
Una vez que observamos más datos, usamos la regla de Bayes para determinar
o actualizar nuestras creencias de posibles parámetros y modelos.

En escencia, el proceso se reduce a calcular:

$$p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)}$$

donde:

<div class="caja">

1. Antes de observar datos $x$, cuantificamos la información (o incertidumbre) acerca del parámetro 
desconocido $\theta$ mediante la **distribución de probabilidad inicial** o *a priori* $p(\theta)$. Esto es, la distribución a priori resume nuestras creencias acerca del parámetro ajenas a los datos.  

2. Por otra parte, cuantificamos la información de $\theta$ asociada a $x$ mediante la 
__función de verosimilitud__ $p(x|\theta) = \mathcal{L}(\theta)$.

3. Combinamos la información a priori y la información que provee $x$ mediante el 
__teorema de Bayes__ obteniendo así la __distribución posterior__ 
$p(\theta|x) \propto p(x|\theta)p(\theta)$.

4. La **evidencia** $p(x)$ es la probabilidad de los datos dado el modelo; se determina sumando (caso discreto) o integrando (caso contínuo) $p(x|\theta)p(\theta)$ a través de todos los valores de los parámetros. 

5. Las inferencias se obtienen de resúmenes de la distribución posterior.
</div>

**Ejemplo: Ingesta calórica de estudiantes.** Supongamos que nos interesa aprender los hábitos alimenticos de los estudiantes
universitarios en México, y escuchamos que de acuerdo a investigaciones se
recomienda que un adulto promedio ingiera 2500 kcal. 

Buscamos conocer
qué proporción de los estudiantes siguen esta recomendación. Para ello 
tomaremos una muestra aleatoria de estudiantes del ITAM. 

Denotemos por $\theta$ 
la proporción de estudiantes que ingieren en un día 2500 kcal o más. El valor 
de $\theta$ es desconocido, y desde el punto de vista bayesiano cuando tenemos 
incertidumbre de algo (puede ser un parámetro o una predicción) lo vemos como 
una variable aleatoria y por tanto tiene asociada una distribución de 
probabilidad que actualizaremos conforme obtenemos información (observamos 
datos).

1. La distribución a priori $p(\theta)$ representa nuestras creencias de los posibles valores que puede 
tomar el parámetro. Supongamos que tras leer artículos y entrevistar
especialistas consideramos los posibles valores de $\theta$ y les asigmanos 
pesos:

```{r}
theta <- seq(0.05, 0.95, 0.1)
pesos.prior <- c(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0, 0)
prior <- pesos.prior/sum(pesos.prior) 
prior_df <- data_frame(theta, prior = round(prior, 3))
prior_df
```

2. Una vez que cuantificamos nuestro conocimiento (o la falta de este) sobre los 
posibles valores que puede tomar $\theta$ especificamos la función de verosimilitud $p(x| \theta)$, donde $x = (x_1,...,x_n)$. Asumiremos que para cada uno de los $n$ estudiantes:
$$p(x_i|\theta) \sim Bernoulli(\theta), \quad i=1,...,n$$
es decir, condicional a $\theta$ la probabilidad de que un 
estudiante ingiera más de 2500 calorías es $\theta$ y la función de 
verosimilitud $p(x_1,...,x_N|\theta) = \mathcal{L}(\theta)$ es:

$$p(x_1,...,x_n|\theta) = \prod_{i=1}^n p(x_i|\theta)= \theta^z(1 - \theta)^{n-z}$$

donde $z$ denota el número de estudiantes que ingirió al menos 2500 kcal y $n-z$ 
el número de estudiantes que ingirió menos de 2500 kcal. 

3. Ahora calculamos la distribución posterior de $\theta$ 
usando la regla de Bayes:

$$p(\theta|x) = \frac{p(x_1,...,x_N|\theta)p(\theta)}{p(x)} \propto  p(\theta)\mathcal{L}(\theta)$$

4. El denominador $p(x)$ no depende de 
$\theta$; está ahí para normalizar la distribución posterior asegurando que tengamos una distribución 
de probabilidad.


#### Distribución inicial discreta
Continuando con el ejemplo de ingesta calórica de estudiantes universitarios, usamos la inicial discreta que discutimos (tabla 
de pesos normalizados) y supongamos que tomamos una muestra de 30 alumnos de 
los cuales $z=11$ ingieren al menos 2500 kcal. Calculemos la distribución 
posterior de $\theta$, con $0<\theta<1$:

```{r, fig.height=2.6, fig.width=5}
library(LearnBayes)
N <- 30 # estudiantes
z <- 11 # éxitos

# Verosimilitud
Like <- theta ^ z * (1 - theta) ^ (N - z)
product <- Like * prior

# Distribución posterior (normalizamos)
post <- product / sum(product)

dists <- bind_cols(prior_df, post = post)
round(dists, 3)

# También podemos usar la función pdisc
pdisc(p = theta, prior = prior, data = c(z, N - z)) %>% round(3)

# Alargamos los datos para graficar
dists_l <- dists %>% 
  gather(dist, val, prior:post)

ggplot(dists_l, aes(x = theta, y = val)) +
  geom_bar(stat = "identity", fill = "darkgray") + 
  facet_wrap(~ dist) +
  labs(x = expression(theta), y = expression(p(theta))) 
```

**Ejercicio:** ¿Cómo se ve la distribución posterior si tomamos 
una muestra de tamaño 90 donde observamos la misma proporción de éxitos. 
Realiza los cálculos y graficala como un panel adicional de la gráfica 
anterior.

**Pregunta:** ¿Cómo definirías la distribución inicial si no tuvieras conocimiento de los
artículos y expertos?


#### Evidencia
Ahora, en el teorema de Bayes también encontramos el término $p(x)$ que 
denominamos la evidencia, esta última también se conoce como 
**verosimilitud marginal** o **inicial predictiva**. 

La evidencia es la probabilidad de los datos de acuerdo al modelo y se calcula
sumando o integrando $p(x, \theta)$ a lo largo de todos los posibles valores de los parámetros $\theta$.

Es importante notar que hablamos de valores de los parámetros $\theta$ 
únicamente en el contexto de un modelo particular $M$ pues éste es el que da sentido a
los parámetros. Podemos hacer evidente el modelo en la notación, 

$$p(\theta|x,M)=\frac{p(x|\theta,M)p(\theta|M)}{p(x|M)}$$

En este contexto la evidencia se define como:
$$p(x|M)=\sum_{\theta} p(x|\theta,M)p(\theta|M)$$
en el caso discreto o,
$$p(x|M)=\int p(x|\theta,M)p(\theta|M)d\theta$$
en el caso contínuo.

La notación anterior es conveniente sobre todo cuando estamos considerando
más de un modelo y queremos usar los datos para determinar la certeza que 
tenemos en cada modelo. 

Supongamos que tenemos dos modelos $M_1$ y $M_2$, 
entonces podemos calcular el cociente de $p(M_1|x)$ y $p(M_2|x)$ obteniendo:

$$\frac{p(M_1|x)}{p(M_2|x)} = \frac{p(x|M_1) \cdot p(M_1)}{p(x|M_2)\cdot p(M_2)}$$

El cociente de evidencias $\frac{p(x|M_1)}{p(x|M_2)}$ se conoce como **factor de
Bayes**.


#### Invarianza en el orden de los datos

Vimos que la regla de Bayes nos permite pasar del conocimiento inicial 
$p(\theta)$ al posterior $p(\theta|x)$ conforme recopilamos datos. Supongamos 
ahora que observamos
más datos, los denotamos $x'$, podemos volver a actualizar nuestras creencias
pasando de $p(\theta|x)$ a $p(\theta|x,x')$. Entonces podemos preguntarnos si 
nuestro conocimiento posterior cambia si actualizamos de acuerdo a $x$ primero
y $x'$ después o vice-versa. La respuesta es que si $p(x|\theta)$ y 
$p(x'|\theta)$ son _iid_ entonces el orden en que actualizamos nuestro 
conocimiento no afecta la distribución posterior. Esta propiedad se denomina **invarianza al orden**.

La invarianza al orden tiene sentido intuitivamente: Si la función de 
verosimilitud no depende del tiempo o del ordenamineto de los datos, entonces
la posterior tampoco tiene porque depender del ordenamiento de los datos.

<!--
#### Pasos de un análisis de datos bayesiano

Como vimos en los ejemplos, en general un análisis de datos bayesiano sigue los
siguientes pasos:

1. Identificar los datos releventes a nuestra pregunta de investigación, el tipo 
de datos que vamos a describir, que variables se van a predecir, que variables 

2. Definir el modelo descriptivo para los datos. La forma matemática y los 
parámetros deben ser apropiados para los objetivos del análisis.

3. Especificar la distribución inicial de los parámetros.

4. Utilizar inferencia bayesiana para reubicar la credibilidad a lo largo de 
los posibles valores de los parámetros.

5. Verificar que la distribución posterior replique los datos de manera 
razonable, de no ser el caso considerar otros modelos descriptivos para los datos.

En el caso de un volado, los datos consisten en águilas y soles, y para el 
modelo descriptivo necesitamos una expresión del función de verosimilitud:

$p(x|\theta)=\theta^x(1-\theta)^{1-x}$

esto es, $x$ tiene una distribución $Bernoulli(\theta)$.

El siguiente paso es determinar la distribución inicial sobre el espacio de
parámetros, como ejemplo, supongamos que solo hay 3 valores de $\theta$ que consideramos $\theat \in \{(0.25, 0.5, 0.75)\}$, y asignamos las probabilidades
$p(\theta = 0.25) = 0.25$, $p(\theta = 0.5) = 0.5$ y $p(\theta = 0.75) = 0.25$

```{r}
theta <- c(0.25, 0.5, 0.75)
prior <- data_frame(theta, p = c(0.25, 0.5, 0.25))
```

El siguiente paso es recolectar los datos y utilizar la regla de Bayes para 
reubicar nuestras creencias a lo largo de los posibles valores. Supongamos 
que observamos un único volado y resulta en águila.

-->

## Objetivos de la inferencia Bayesiana

Los tres objetivos de la inferencia son: estimación de parámetros, predicción
de valores y comparación de modelos.

1. La **estimación de parámetros** implica determinar hasta que punto creemos en 
cada posible valor del parámetro. La distribución posterior sobre los valores 
de los parámetros $\theta$ es nuestra estimación para dichos valores.

**Ejemplo:** Considere que proponemos los siguientes dos modelos (distribución inicial y función de verosimilitud) para una muestra que consta de una susesión de $N$ lanzamientos de moneda en la que salieron $z$ águilas:
```{r, fig.height=2.8, fig.width=7.3}
# Modelo 1, creamos una inicial que puede tomar 3 valores
p_M1 <- data_frame(theta = c(0.25, 0.5, 0.75), prior = c(0.25, 0.5, 0.25), 
  modelo ="M1")
p_M1

# Modelo 2, Creamos una inicial que puede tomar 51 valores
p <- seq(0, 24, 1)
p
p2 <- c(p, 25, sort(p, decreasing = TRUE))
p2
p_M2 <- data_frame(theta = seq(0, 1, 0.02), prior = p2 / sum(p2), 
  modelo = "M2")
p_M2

#Parámetros
N <- 20 # estudiantes
z <- 12 # éxitos

dists_h <- bind_rows(p_M1, p_M2) %>% # base de datos horizontal
    group_by(modelo) %>%
    mutate(
        Like = theta ^ z * (1 - theta) ^ (N - z), # verosimilitud 
        posterior = (Like * prior) / sum(Like * prior)
      ) 
dists_h

dists <- dists_h %>% # base de datos larga
    gather(dist, valor, prior, Like, posterior) %>% 
    mutate(dist = factor(dist, levels = c("prior", "Like", "posterior")))
dists

ggplot(filter(dists, modelo == "M1"), aes(x = theta, y = valor)) +
    geom_bar(stat = "identity", fill = "darkgray") +
    facet_wrap(~ dist, scales = "free") +
    scale_x_continuous(expression(theta), breaks = seq(0, 1, 0.2)) +
    labs(y = "")

ggplot(filter(dists, modelo == "M2"), aes(x = theta, y = valor)) +
    geom_bar(stat = "identity", fill = "darkgray") +
    facet_wrap(~ dist, scales = "free") +
    scale_x_continuous(expression(theta), breaks = seq(0, 1, 0.2)) +
    labs(y = "")

```

2. **Predicción** de valores. Usando nuestro conocimiento actual nos interesa
predecir la probabilidad de datos futuros:

Si nuestro conocimiento actual consta únicamente de las creencias iniciales de la distribución inical $p(\theta)$, utilizamos la distribución predictiva inicial:
$$p(y) =\sum_{\theta} p(y|\theta)p(\theta)$$
en el caso discreto, o
$$p(y) =\int p(y|\theta)p(\theta)d\theta$$
en el caso contínuo.

Notese que la ecuación anterior coincide con la correspondiente a la evidencia, 
con la diferencia de que la evidencia se refiere a un valor observado y en 
esta ecuación estamos calculando la probabilidad de cualquier valor $y$.

Si nuestro conocimiento actual involucra datos observados $x$, utilizamos la distribución predictiva posterior:
$$p(y|x) =\sum_{\theta} p(y|\theta)p(\theta|x)$$
en el caso discreto, o
$$p(y|x) =\int p(y|\theta)p(\theta|x)d\theta$$
en el caso contínuo.

Por ejemplo podemos usar las creencias iniciales del modelo 1, que propusimos 
arriba para calcular la probabilidad predictiva de observar águila:

$$p(y=A) = \sum_{\theta}p(y=A|\theta)p(\theta) = 0.5$$
Vale la pena destacar que las prediciones son probabilidades de cada posible
valor condicional a nuestro modelo de creencias actuales. Si nos interesa 
predecir un valor particular en lugar de de una distribución a lo largo de todos
los posibles valores podemos usar la media de la distribución predictiva. Por 
tanto el valor a predecir sería (en el caso contínuo):

$$E[y|x]=\int y \ p(y|x) dy$$

La integral anterior únicamente tiene sentido si $y$ es una variable continua. 
Si $y$ es nominal, entonces podemos usar el valor más probable.


**Ejercicio:** Calcula las probabilidades predictivas para $y=0,1$ de los modelos 1 y 2 del ejemplo anterior usando la distribución posterior de cada modelo.


```{r, echo=FALSE, eval=FALSE}
dists_h %>%
  group_by(modelo) %>%
  summarise(A = sum(theta * posterior))

dists_h %>%
  group_by(modelo) %>%
  summarise(A = sum(theta * prior))
```

3. **Comparación de modelos**. Una caracterítica conveniente de la comparación 
de modelos en estadística Bayesiana es que la complejidad del modelo se toma
en cuenta de manera automática. 

**Ejemplo:** El primero modelo del ejemplo anterior supusimos que el parámetro
$\theta$ únicamente puede tomar uno de 3 valores (0.25, 0.5, 0.75); esta 
restricción dió lugar a un modelo simple. Por su parte, el modelo 2 es más 
complejo y permite muchos más valores de $\theta$ (51). La forma de la 
distribución inicial es triangular en ambos casos, el valor de mayor 
probabilidad inicial es $\theta = 0.50$ y reflejamos que creemos que es menos
factible que el valor se encuentre en los extremos.

Podemos calcular el factor de Bayes para distintos datos observados:

```{r}
factorBayes <- function(N, z){
  evidencia <- bind_rows(p_M1, p_M2) %>% # base de datos horizontal
    group_by(modelo) %>%
    mutate(
      Like = theta ^ z * (1 - theta) ^ (N - z), # verosimilitud 
      posterior = (Like * prior) / sum(Like * prior)
    ) %>%
    summarise(evidencia = sum(prior * Like))
  print(evidencia)
  return(evidencia[1, 2] / evidencia[2, 2])
}

factorBayes(50, 25)
factorBayes(100, 75)
factorBayes(100, 10)
factorBayes(40, 38)
```

¿Cómo explicarías los resultados anteriores? En el modelo complejo $\theta$ 
puede tomar más valores, entonces si en una sucesión de volados observamos 
10% de águilas, el modelo simple no cuenta con un valor de $\theta$ cercano al 
resultado observado, pero el modelo complejo si. Por otra parte, para valores
de $\theta$ que se encuentran en ambos modelos la probabilidad inicial de esos 
valores es mayor en el caso del modelo simple. Por lo tanto si los datos 
observados resultan en valores de $\theta$ congruentes con el modelo simple, 
creeríamos en el modelo simple más que en el modelo más complicado.

Es importante destacar que la comparación de modelos nos habla 
únicamente de la evidencia relativa de dos modelos; sin embargo, puede que
ninguno de los modelos que estamos considerando sean adecuados para nuestros
datos, por lo que más adelante estudiaremos maneras de evaluar un modelo.


## Calculo de la distribución posterior

En la inferencia Bayesiana se requiere calcular el denominador de la regla
de Bayes $p(x)$; es común que esto requiera que se calcule una integral 
complicada, que en ocasiones no tenga una forma cerrada. Sin embargo, hay algunas maneras de evitar esto:

1. El camino tradicional consiste en usar funciones de verosimilitud con 
distribuciones iniciales conjugadas. Cuando una distribución inicial es 
**conjugada** de la función de verosimilitud, la integral resulta en una distribución posterior con la 
misma forma funcional que la distribución inicial.

2. Otra alternativa es aproximar la integral numéricamente. Cuando el espacio de
parámetros es de dimensión chica, se puede aproximar con una cuadrícula de 
puntos y la integral se puede calcular sumando a través de dicha cuadrícula. 
Sin embargo, cuando el espacio de parámetros aumenta de dimensión el número de
puntos necesarios para la aproximación crece demasiado y hay que recurrir a otas
técnicas.

3. Se han desarrollado una clase de métodos de simulación para poder calcular 
la distribución posterior, estos se conocen como cadenas de Markov via Monte
Carlo (MCMC por sus siglas en inglés). El desarrollo de los métodos MCMC es lo
que ha propiciado el desarrollo de la estadística bayesiana en años recientes.


Para ilustrar estas 3 aproximaciones a la integral que define la distribución posterior, consideraremos el ejemplo de una distribución Bernoulli.

### Distribuciones conjugadas

Comenzaremos con el modelo Beta-Binomial. Recordemos que si $X$ es una variable aleatoria 
con dos posibles resultados, entonces $X$ se distribuye Bernoulli y la función de 
probabilidad esta definida por:

$$p(x|\theta)=\theta^x(1-\theta)^{1-x}$$

Usualmente pensamos en la fórmula anterior como una función 
de $x$, donde $x$ toma uno de dos valores: 0 o 1. También podemos pensar que $x$
esta fija (observada) y la expresión es una función de $\theta$, en este caso, la 
ecuación especifica la probabilidad de un valor fijo $x$ para un valor de 
$\theta$ y la llamamos la función de verosimilitud de $\theta$. En inferencia Bayesiana, la 
función $p(x|\theta)$ usualmente se considera como función de $\theta$.

Ahora, si lanzamos una moneda $N$ veces tenemos un conjunto de datos 
$\{x_1,...,x_N\}$. Si suponemos que los lanzamientos son *i.i.d.*, la probabilidad de observar el conjunto de $N$ lanzamientos es el 
producto de las probabilidades de cada observación:

$$p(x_1,...,x_N|\theta) = \prod_{n=1}^N p(x_n|\theta)$$
$$= \theta^z(1 - \theta)^{N-z}$$
donde $z$ denota el número de éxitos (águilas).

Ahora, en principio, para describir nuestras creencias iniciales podríamos usar
cualquier función de densidad de probabilidad inicial $p(\theta)$ con soporte en $[0, 1]$. Sin embargo, sería
conveniente que el producto $p(x|\theta)p(\theta)$ (el numerador de la fórmula 
de Bayes)
resulte en una función con la misma forma funcional que $p(\theta)$. Cuando éste es el 
caso, las distribuciones inicial y posterior se describen con la misma fisma familia de distribuciones.
Esto es conveninte pues si obtenemos nueva información podemos actualizar 
nuestro conocimiento de manera inmediata, conservando la forma de las 
distribuciones.

<div class="caja">
Cuando las funciones $p(x|\theta)$ y $p(\theta)$ se combinan de tal manera
que la distribución posterior pertenece a la misma familia (tiene la misma
forma funcional) que la distribución inicial, entonces decimos que $p(\theta)$ es 
**conjugada** con $p(x|\theta)$. 
</div>

<br/>
Vale la pena notar que la distribución inicial es conjugada con una 
función de verosimilitud particular.

Una distribución conjugada con $p(x|\theta) = \theta^z(1 - \theta)^{N-z}$ es 
una $Beta(a, b)$
$$p(\theta) = \frac {\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)}$$

Para describir nuestro conocimiento inicial podemos explorar la media y 
la varianza de la distribución beta. La media es 
$$\bar{\theta} = a/(a+b)$$
por lo que si $a=b$ la media es 0.5 y conforme 
aumenta $a$ en relación a $b$ aumenta la media. La varianza es:

$$s^2 = \frac{\bar{\theta}(1-\bar{\theta})}{(a+b+1)}$$

Una manera de seleccionar los parámetros $a$ y $b$ es pensar en la 
proporción media de águilas (m) y el tamaño de la muestra (n). Ahora, $m=a/(a+b)$
y $n = a+b$, por lo que obtenemos

$$a=mn, \quad b=(1-m)n$$

Otra manera es comenzar con la media y la varianza. Al usar este 
enfoque debemos recordar que la desviación estándar $s$ debe tener sentido en el 
contexto de la densidad beta. En particular la desviación estándar típicamente
es menor a 0.289, la cual corresponde a la desviación estándar de una distribución uniforme contínua.
Entonces, para una distribución beta con media $\bar{\theta}$ y varianza $s^2$, los
parámetros son:
$$a=\bar{\theta}\bigg(\frac{\bar{\theta}(1-\bar{\theta})}{s^2}- 1\bigg), \quad b=(1-\bar{\theta})\bigg(\frac{\bar{\theta}(1-\bar{\theta})}{s^2}- 1\bigg)$$

Una vez que hemos determinado una distribución inicial conveniente para la función de verosimilitud 
Bernoulli, calculamos la distribución posterior. 

Supongamos que observamos $N$ lanzamientos de 
los cuales $z$ son águilas, entonces podemos ver que la distribución posterior es nuevamente
una densidad Beta.

$$p(\theta|z)\propto \theta^{a+z-1}(1 -\theta)^{(N-z+b)-1} = p(x|\theta) p(\theta)$$

Concluímos entonces que si la distribución inicial es $Beta(a,b)$, la posterior
es $Beta(z+a,N-z+b)$.

Vale la pena explorar la relación entre las medias de la distribución inicial y la distribución posterior. La media incial es 
$$\frac{a}{(a+b)}$$
y la media posterior es 
$$\frac{(z+a)}{(z+a) + (N-z+b)}=\frac{z+a}{N+a+b}$$
Podemos hacer algunas manipulaciones algebráicas para escribirla como:

$$\frac{z+a}{N+a+b}=\frac{z}{N}\frac{N}{N+a+b} + \frac{a}{a+b}\frac{a+b}{N+a+b}$$

es decir, podemos escribir la media posterior como un promedio ponderado entre
la media inicial $a/(a+b)$ y la proporción observada $z/N$.

1. **Inferencia**: Ahora podemos pasar a la inferencia. La distribución posterior resume todo 
nuestro conocimiento del parámetro $\theta$ (la proporción), en este caso podemos graficar la 
distribución posterior y extraer valores numéricos como la media.

**Ejemplo:** Consideremos dos distribuciones iniciales beta diferentes:
```{r, fig.height=3, fig.width=6}
N = 14; z = 11; a = 1; b = 1
base <- ggplot(data_frame(x = c(0, 1)), aes(x)) 

p1 <- base +
    stat_function(fun = dbeta, args = list(shape1 = a, shape2 = b), 
        aes(colour = "inicial"), show.legend = FALSE) + 
    stat_function(fun = dbeta, args = list(shape1 = z + 1, shape2 = N - z + 1), 
        aes(colour = "verosimilitud"), show.legend = FALSE) + 
    stat_function(fun = dbeta, args = list(shape1 = a + z, shape2 = N - z + b), 
        aes(colour = "posterior"), show.legend = FALSE) +
      labs(y = "", colour = "", x = expression(theta))

N = 14; z = 11; a = 100; b = 100
p2 <- base +
    stat_function(fun = dbeta, args = list(shape1 = a, shape2 = b), 
        aes(colour = "inicial")) + 
    stat_function(fun = dbeta, args = list(shape1 = z + 1, shape2 = N - z + 1), 
        aes(colour = "verosimilitud")) + 
    stat_function(fun = dbeta, args = list(shape1 = a + z, shape2 = N - z + b), 
        aes(colour = "posterior")) +
      labs(y = "", colour = "", x = expression(theta))

grid.arrange(p1, p2, nrow = 1, widths = c(0.38, 0.62))
```



<!--
Este html puede ser interactivo, para ver la aplicación de shiny debes correr el 
Rmd de manera local (*Run Document*). 

```{r shiny_bernoulli, echo=FALSE, error=TRUE, cache=FALSE, eval = FALSE}
shinyAppDir("app_bernoulli", options = list(width = "100%", height = 500))
```

-->

Una manera de resumir la distribución posterior es a través de intervalos de 
probabilidad. En inferencia Bayesiana, los intervalos de probabilidad se utilizan para establecer qué valores del parámetro son creíbles, por lo que se denominan **intervalos de credibilidad**.

**Ejercicio:** Calcula un intervalo de credibilidad del 95% para
cada una de las distribuciones posteriores del ejemplo anterior.

```{r, echo=FALSE, eval=FALSE}
qbeta(c(0.025, 0.975), shape1 = 12, shape2 = 4)
qbeta(c(0.025, 0.975), shape1 = 111, shape2 = 103)
```

2. **Predicción**: Ahora pasemos a predicción. Calculamos la probabilidad de $y =1$:

$$p(y = 1|x) = \int p(y=1|\theta)p(\theta|x)d\theta$$
$$=\int \theta \ p(\theta|z,N) d\theta$$
$$=\frac{z+a}{N+a+b}$$
Esto es, la probabilidad predictiva de águila es la media de la distribución
posterior sobre $\theta$. 

3. **Comparación de modelos**: Finalmente, comparemos modelos. Para esto calculamos la evidencia $p(x|M)$ para cada modelo:

$$p(x|M)=\int p(x|\theta,M)p(\theta|M)d\theta$$

En este caso, los datos se resumen mediante las cantidades $z$ y $N$. Para una distribución incial beta
es fácil calcular la evidencia:

$$p(x|M) \equiv p(z,N)=\frac{B(z+a,N-z+b)}{B(a,b)}$$

En nuestro ejemplo:
```{r}
# N = 14, z = 11, a = 1, b = 1
P_M1 <- beta(12, 4) / beta(1, 1)
P_M1
# N = 14, z = 12, a = 100, b = 100 
P_M2 <- beta(126, 126) / beta(100, 100)
P_M2
factor_Bayes <- P_M1 / P_M2
factor_Bayes
```
Vemos que el modelo 1 se ajusta mucho mejor a los datos: la distribución inicial del modelo 2 tiene un pico en 0.5 mientras que la otra es uniforme. Debido a que la proporción de águilas observadas en la muestra no es cercana a 0.5, la distribución inicial picuda (modelo 2) no captura bien los datos.
<!--
Supongamos que observamos una secuencia en la que la mitad de los volados
resultan en águila:

```{r}
# N = 14, z = 7, a = 1, b = 1
beta(8, 8) / beta(1, 1)
# N = 14, z = 7, a = 100, b = 100 
beta(107, 107) / beta(100, 100)
```
-->

En general, preferimos un modelo con un valor mayor de $p(x|\theta)$, pero la 
preferencia no es absoluta ya que una diferencia chica no nos dice mucho; debemos
considerar que los datos no son mas que una muestra aleatoria.

**Ejercicio:** Supongamos que nos interesa analizar el IQ de una
muestra de estudiantes del 
ITAM y suponemos que el IQ de un estudiante tiene una distribución normal 
$x \sim N(\theta, \sigma^2)$ con $\sigma ^ 2$ conocida.

Considera que observamos el IQ de un estudiante $x$. 
La función de verosimilitud del modelo es:
$$p(x|\theta)=\frac{1}{\sqrt{2\pi\sigma^2}}exp\left(-\frac{1}{2\sigma^2}(x-\theta)^2\right)$$

Realizaremos un análisis bayesiano, por lo que hace falta establecer una 
distribución inicial. Elegiremos que $p(\theta)$ se distribuya $N(\mu, \tau^2)$ 
donde elegimos los parámetros $\mu, \tau$ que mejor describan nuestras creencias
iniciales.

Calcula la distribución posterior $p(\theta|x) \propto p(x|\theta)p(\theta)$, 
utilizando la distribución inicial y la función de verosimilitud que definimos arriba. Una vez que realices la
multiplicación debes identificar el núcleo de una distribución Normal, 
¿cuáles son los parámetros (media y varianza) de la distribución posterior?


### Aproximación por cuadrícula

Supongamos que la distribución beta no describe nuestras creencias de manera
adecuada. Por ejemplo, mis creencias podrían estar mejor representadas por una
distribución trimodal: con tres máximos (ej. la moneda está fuertemente sesgada hacia sol, 
fuertemente sesgada hacia águila o es justa). No hay parámetros en una beta
que puedan describir este patrón, y probablemente no esposible realizar la integración analítica para encontrar la distribución posterior con una función que describe una distribución inicial de este tipo.

Exploraremos entonces una técnica de aproximación numérica de la distribución 
posterior que consiste en definir la distribución inicial en una cuadrícula
de valores de $\theta$. En este método no necesitamos describir nuestras 
creencias mediante una función matemática ni realizar integración analítica.

Suponemos que existe únicamente un número finito de valores de $\theta$ que 
creemos que pueden ocurrir (en el primer ejemplo que estudiamos usamos esta 
técnica). Es así que la regla de Bayes se escribe como:

$$p(x|\theta)=\frac{p(x|\theta)p(\theta)}{\sum_{\theta}p(x|\theta)p(\theta)}$$

Esta técnica nos permite aproximar la integral que aparecería en el denominador anterior discretizando una distribución inicial continua mediante una
cuadrícula de masas de probabilidad discreta y por tanto podemos usar la versión discreta 
de la regla de Bayes. El proceso consiste en dividir el dominio en regiones, 
crear un rectángulo con la altura correspondiente al valor de la densidad en 
el punto medio. Aproximamos el área de cada región mediante la altura del 
rectángulo.

__Ejemplo: Distribución inicial contínua uniforme.__ Si discretizamos la distribución considerando 20 valores del parámetro $\theta$ obtenemos:
```{r, fig.height=3, fig.width=7.5}
# N = 14, z = 11, a = 1, b = 1
N = 14; z = 11
inicial <- data.frame(theta = seq(0.05, 1, 0.05), inicial = rep(1/20, 20))
dists_h <- inicial %>%
    mutate(
        verosimilitud = theta ^ z * (1 - theta) ^ (N - z), # verosimilitud 
        posterior = (verosimilitud * inicial) / sum(verosimilitud * inicial)
        )  
dists <- dists_h %>% # base de datos larga
    gather(dist, valor, inicial, verosimilitud, posterior) %>% 
    mutate(dist = factor(dist, levels = c("inicial", "verosimilitud", "posterior")))

ggplot(dists, aes(x = theta, y = valor)) +
    geom_point() +
    facet_wrap(~ dist, scales = "free") +
    scale_x_continuous(expression(theta), breaks = seq(0, 1, 0.2)) +
    labs(y = "")
```

y lo podemos comparar con la versión continua (distribución beta con $a=b=1$):

```{r, echo=FALSE, fig.height=3, fig.width=7.5}
plot1 <- base + 
    stat_function(fun = dbeta, args = list(shape1 = 1, shape2 = 1)) + # inicial
    labs(x = "", y = "", title = "inicial")
plot2 <- base +
    stat_function(fun = dbeta, args = list(shape1 = 12, shape2 = 4)) + # verosimilitud
    labs(x = expression(theta), y = "", title = "verosimilitud")
plot3 <- base +
    stat_function(fun = dbeta, args = list(shape1 = 12, shape2 = 4)) + # posterior
    labs(x = "", y = "", title = "posterior")
grid.arrange(plot1, plot2, plot3, ncol=3)
```


1. **Inferencia:** En cuanto a la estimación, la tabla de probabilidades nos da una estimación
para los valores de los parámetros. 

Podemos calcular resúmenes estadísticos de la distribución de $\theta$ de manera aproximada. Por ejemplo, la media se aproxima mediante promedio ponderado por las probabilidades:

$\bar{\theta}=\sum_{\theta} \theta p(\theta|x)$

```{r}
head(dists_h)
sum(dists_h$posterior * dists_h$theta)
```

En cuanto a los intervalos de probabilidad, debido a que estamos usando masas 
discretas, la suma de las masas en un intervalo usualmente no será exactamente igual al coeficiente de confianza. Para un coeficiente de confianza de 95%, tenemos que elegir los puntos tales que la masa sea mayor a igual 
a 95% y la masa total sea lo menor posible. En nuestro ejemplo podemos usar
cuantiles:

```{r}
dist_cum <- cumsum(dists_h$posterior) #vector de distribución acumulada
lb <- which.min(dist_cum < 0.05) - 1
ub <- which.min(dist_cum < 0.975)
dists_h$theta[lb]
dists_h$theta[ub]
```

2. **Predicción:** Para el problema de predicción, la probabilidad predictiva para el 
siguiente valor $y$ es simplemente la probabilidad de que ocurra dicho valor
ponderado por la probabilidad posterior correspondiente:

$$p(y|x)=\int p(y|\theta)p(\theta|x)d\theta$$
$$\approx \sum_{\theta} p(y|\theta)p(\theta|x)$$

**Ejercicio:** Calcula la probabilidad predictiva para $y=1$
usando los datos del ejemplo anterior.

3. **Comparación de Modelos:** Finalmente, para la comparación de modelos la integral que define la evidencia

$$p(x|M)=\int p(x|\theta,M)p(\theta|M)d\theta$$

se convierte en una suma

$$p(x|M)\approx \sum_{\theta} p(x|\theta,M)p(\theta|M)d\theta$$

Para el ejemplo del experimento Bernoulli donde definimos los modelos $M_1$ y $M_2$ tendríamos:
```{r}
# calcula el factor de Bayes para el experimento Bernoulli Modelos M1 y M2
factorBayes <- function(M, s){
  evidencia <- rbind(p_M1, p_M2) %>% # base de datos horizontal
    group_by(modelo) %>%
    mutate(
      Like = theta ^ s * (1 - theta) ^ (M - s), # verosimilitud 
      posterior = (Like * prior) / sum(Like * prior)
    ) %>%
    summarise(evidencia = sum(prior * Like))
  print(evidencia)
  return(evidencia[1, 2] / evidencia[2, 2])
}

factorBayes(50, 25)
```


### Algoritmo de Metrópolis

Hay ocasiones en las que los métodos de inicial conjugada y aproximación por
cuadrícula no funcionan; hay casos en los que la distribución conjugada no describe
nuestras creencias iniciales o la aproximación numérica no es factible debido al gran número de parámetros involucrados. Es por ello que surge la necesidad de
utilizar métodos de Monte Carlo vía Cadenas de Markov (MCMC).

El algoritmo de Metropolis asume que podemos calcular $p(\theta)$ para un valor
particular de $\theta$ y el valor de la verosimilitud $p(x|\theta)$ para 
cualquier $x$, $\theta$ dados. En realidad, el método únicamente requiere que
se pueda calcular el producto de la distribución inicial y la función de verosimilitud. Lo que el método produce es una 
aproximación de la distribución posterior $p(\theta|x)$ mediante una 
muestra de valores de $\theta$ obtenido de dicha distribución.

**Ejemplo: Caminata aleatoria**. Con el fin de entender el algoritmo comenzaremos 
estudiando el concepto de caminata aleatoria a través de un ejemplo. 

Supongamos que un vendedor de 
enciclopedias trabaja a lo largo de una cadena finita de islas. Constantemente viaja
entre las islas ofreciendo sus productos. Al final de un día de trabajo decide 
si permanece en la misma isla o se transporta a una de las 2 islas vecinas. El 
vendedor desconoce a priori la distribución de la población en las islas; sin embargo, una 
vez que se encuentra en una isla puede investigar la población de la misma y 
también de la isla a la que se propone viajar después. 

El objetivo del vendedor es visitar las islas de manera proporcional a la 
población de cada una. Con esto en mente el vendedor utiliza el siguiente 
proceso: 

1. Lanza un volado, si el resultado es águila se propone ir a la isla 
del lado izquierdo de su ubicación actual y si es sol a la del lado derecho.

2. Si la isla propuesta en el paso anterior tiene población mayor a la población
de la isla actual, el vendedor decide viajar a ella. Si la isla tiene
población menor, entonces visita la isla propuesta con una probabilidad que 
depende de la población relativa de la isla propuesta y la isla actual. 

Sea $\theta$ el número de isla. Sea $P(\theta_{prop})$ la población de la isla 
propuesta y $P(\theta_{actual})$ la población de la isla actual. Entonces el vendedor
cambia de isla con probabilidad $p_{mover}=P(\theta_{prop})/P(\theta_{actual})$.

A la larga, si el vendedor sigue la heurística anterior la probabilidad de que
el vendedor se encuentre en alguna de las islas $P(\theta)$ coincide con la población relativa (normalizada) de la isla. 

Consideremos el caso en el que se tienen 10 islas y la diferencia de población entre islas consecutivas es constante:
```{r, fig.height=6, fig.width=3.5}
islas <- data.frame(islas = 1:10, pob = 1:10)

caminaIsla <- function(i){ # i: isla actual
  u <- runif(1) # volado
  v <- ifelse(u < 0.5, i - 1, i + 1)  # isla vecina (índice)
  if(v < 1 | v > 10){ # si estas en los extremos y el volado indica salir
    return(i)
  }
  u2 <- runif(1)
  p_move = min(islas$pob[v] / islas$pob[i], 1)
  if(p_move  > u2){
    return(v) # isla destino
  }
  else{
    return(i) # me quedo en la misma isla
  }
}

pasos <- 100000
camino <- numeric(pasos)
camino[1] <- sample(1:10, 1) # isla inicial
for(j in 2:pasos){
  camino[j] <- caminaIsla(camino[j - 1])
}

caminata <- data.frame(pasos = 1:pasos, isla = camino)

plot_caminata <- ggplot(caminata[1: 1000, ], aes(x = pasos, y = isla)) +
  geom_point(size = 0.8) +
  geom_path(alpha = 0.5) +
  coord_flip() + 
  labs(title = "Caminata aleatoria") +
  scale_y_continuous(expression(theta), breaks = 1:10) +
  scale_x_continuous("Tiempo")

plot_dist <- ggplot(caminata, aes(x = isla)) +
  geom_histogram() +
  scale_x_continuous(expression(theta), breaks = 1:10) +
  labs(title = "Distribución objetivo (no normalizada)", 
       y = expression(P(theta)))

grid.arrange(plot_caminata, plot_dist, ncol = 1, heights = c(4, 2))
```

La distribución que deseamos conocer se conoce como **distribución objetivo** $P(\theta)$. En este ejemplo sencillo la distribución objetivo es la propabilidad de que el vendedor se encuentre en una isla, la cual por construcción coincide con la población relativa de cada isla (normalizada). Para obtener una buena aproximación de la distribución objetivo debemos permitir que el vendedor recorra 
las islas durante una sucesión larga de pasos y 
registramos sus visitas (frecuencias relativas). 

Nuestra aproximación de la distribución objetivo es justamente 
el registro de sus visitas (normalizado). Más aún, debemos tener cuidado y excluir la 
porción de las visitas que se encuentran bajo la influencia de la posición 
inicial. Esto es, debemos excluir el **periodo de calentamiento**. 

Una vez que
tenemos un registro _largo_ de los viajes del vendedor (excluyendo el 
calentamiento) podemos aproximar la distribución objetivo $P(\theta)$ de cada valor de 
$\theta$ (el número de isla) simplemente contando el número relativo de veces que el vendedor visitó
dicha isla y normalizando.

```{r, warning=FALSE, message=FALSE, fig.width=8, fig.height=7.5}
t <- c(1:10, 20, 50, 100, 200, 1000, 5000)

plots_list <- lapply(t, function(i){
  ggplot(caminata[1:i, ], aes(x = isla)) +
    geom_histogram() +
    labs(y = "", x = "", title = paste("t = ", i, sep = "")) +
    scale_x_continuous(expression(theta), breaks = 1:10, limits = c(0, 11))
})

args.list <- c(plots_list,list(nrow=4,ncol=4))
do.call(grid.arrange, args.list)
```


Escribamos el algoritmo, para esto indexamos las islas por el valor
$\theta$, es así que la isla del extremo izquierdo corresponde a $\theta=1$ y la 
población relativa normalizada de cada isla es $P(\theta)$:

1. El vendedor de ubica en $\theta_{actual}$ y propone moverse a la izquierda
o derecha con probabilidad 0.5. 

El rango de los posibles valores para moverse, y la probabilidad de proponer 
cada uno se conoce como **distribución propuesta**; en nuestro ejemplo sólo 
toma dos valores cada uno con probabilidad 0.5. 

2. Una vez que se propone un movimiento, decidimos si aceptarlo. La decisión de
aceptar el movimiento se basa en el valor de la distribución objetivo en la posición
propuesta (nuevo valor de $\theta$), relativo al valor de la distribución objetivo en la posición actual (valor actual de $\theta$):

$$p_{mover}=min\bigg( \frac{P(\theta_{propuesta})}{P(\theta_{actual})},1\bigg)$$

Notemos que la distribución objetivo $P(\theta)$ no necesita estar normalizada, 
esto es porque lo que nos interesa es el cociente $P(\theta_{propuesta})/P(\theta_{actual})$.

3. Una vez que propusimos un movimiento y calculamos la probabilidad de aceptar 
el movimiento aceptamos o rechazamos el movimiento generando un valor de una
distribución uniforme, si dicho valor es menor a $p_{mover}$ entonces hacemos
el movimiento.

Entonces, para utilizar el algoritmo necesitamos ser capaces de:

* Generar un valor de la distribución propuesta (para crear $\theta_{propuesta}$).

* Evaluar la distribución objetivo en cualquier valor propuesto (para calcular
$P(\theta_{propuesta})/P(\theta_{actual})$).

* Generar un valor uniforme (para movernos con probabilidad $p_{mover}$)

Las 3 puntos anteriores nos permiten generar muestras aleatorias de la
distribución objetivo, sin importar si esta está normalizada. 

Esta técnica es
particularmente útil cuando la distribución objetivo es la **distribución posterior** $p(\theta|x)$,
proporcional a $p(x|\theta)p(\theta)$.

Para entender porque funciona el algoritmo de Metrópolis hace falta entender 2
puntos:

+ Primero, que la distribución objetivo es **estable**: si la probabilidad
actual de ubicarse en una posición coincide con la probabilidad de la 
distribución objetivo, entonces el algoritmo preserva las probabilidades.

```{r, warning=FALSE, message=FALSE, fig.width=8, fig.height=7.5}
library(expm)

transMat <- function(P){ # recibe vector de probabilidades (o población)
  T <- matrix(0, 10, 10)
  n <- length(P - 1) # número de estados
  for(j in 2:n - 1){ # llenamos por fila
    T[j, j - 1] <- 0.5 * min(P[j - 1] / P[j], 1)
    T[j, j] <- 0.5 * (1 - min(P[j - 1] / P[j], 1)) + 
               0.5 * (1 - min(P[j + 1] / P[j], 1))
    T[j, j + 1] <- 0.5 * min(P[j + 1] / P[j], 1)
  }
  # faltan los casos j = 1 y j = n
  T[1, 1] <- 0.5 + 0.5 * (1 - min(P[2] / P[1], 1))
  T[1, 2] <- 0.5 * min(P[2] / P[1], 1)
  T[n, n] <- 0.5 + 0.5 * (1 - min(P[n - 1] / P[n], 1))
  T[n, n - 1] <- 0.5 * min(P[n - 1] / P[n], 1)
  T
}

T <- transMat(islas$pob)

w <- c(0, 1, rep(0, 8))

t <- c(1:10, 20, 50, 100, 200, 1000, 5000)
expT <- map_df(t, ~data.frame(t = ., w %*% (T %^% .)))
expT_long <- expT %>%
    gather(theta, P, -t) %>% 
    mutate(theta = parse_number(theta))

ggplot(expT_long, aes(x = theta, y = P)) +
  geom_bar(stat = "identity", fill = "darkgray") + 
  facet_wrap(~ t) +
  scale_x_continuous(expression(theta), breaks = 1:10, limits = c(0, 11))
```

+ Segundo, que el proceso **converge** a la distribución objetivo. 
Podemos ver, (en nuestro ejemplo sencillo) que sin importar el punto de inicio
se alcanza la distribución objetivo.

```{r, warning=FALSE, message=FALSE, fig.width=8, fig.height=7.5}
inicioP <- function(i){
  w <- rep(0, 10)
  w[i] <- 1
  t <- c(1, 10, 50, 100)
  expT <- map_df(t, ~data.frame(t = ., inicio = i, w %*% (T %^% .))) %>%
    gather(theta, P, -t, -inicio) %>% 
    mutate(theta = parse_number(theta))
  expT
}

expT <- map_df(c(1, 3, 5, 9), inicioP)
ggplot(expT, aes(x = as.numeric(theta), y = P)) +
  geom_bar(stat = "identity", fill = "darkgray") + 
  facet_grid(inicio ~ t) +
  scale_x_continuous(expression(theta), breaks = 1:10, limits = c(0, 11))

```

Es importante recalcar que el algoritmo converge a la distribución objetivo sin importar si el punto de partida es una sola isla o cualquier distribución de probabilidad de iniciar en diferentes islas.


#### Caso general

En la sección anterior implementamos el algoritmo de Metrópolis en un caso
sencillo: las posiciones eran discretas, en una dimensión y la distribución propuesta involucraba
únicamente mover a la izquierda o a la derecha. El algoritmo general aplica 
para valores continuos, en cualquier número de dimensiones y con distribuciones
propuesta más generales. Lo esencial del método no cambia para el caso general, 
esto es:

1. Tenemos una distribución objetivo $P(\theta)$ de la cual buscamos generar
muestras. Debemos ser capaces de calcular el valor de $P(\theta)$ para cualquier
valor candidato $\theta$. La distribución objetivo $P(\theta)$ no tiene que 
estar normalizada; típicamente $P(\theta)$ es la distribución posterior de 
$\theta$ no normalizada, es decir, es el producto de la verosimilitud y la 
inicial.

2. La muestra de la distribución objetivo se genera mediante una caminata
aleatoria a través del espacio de parámetros. La caminata inicia en un lugar 
arbitrario (definido por el usuario). El punto inicial debe ser tal que 
$P(\theta)>0$. La caminata avanza en cada tiempo proponiendo un movimiento a una
nueva posición mediante una distribución propuesta (1 elemento $\theta$ muestreado) y después decidiendo si se acepta o no el valor propuesto. Las
distribución propuesta puede tener muchas formas, el objetivo es que la 
distribución propuesta explore el espacio de parámetros de manera eficiente.

3. Una vez que tenemos un valor propuesto decidimos si aceptar el movimiento calculando:

$$p_{mover}=min\bigg( \frac{P(\theta_{propuesta})}{P(\theta_{actual})},1\bigg)$$

Si el movimiento no es aceptado, el valor $\theta_{actual}$ se repite en la muestra mientras que el valor $\theta_{propuesta}$ es descartado de la muestra. Al final obtenemos valores representativos de la distribución objetivo $\{\theta_1,...,\theta_n\}$.

Es importante recordar que debemos excluir las primeras observaciones pues 
estas siguen bajo la influencia del valor inicial.

Retomemos el problema de inferencia Bayesiana y veamos como usar el algoritmo
de Metrópolis cuando la distribución objetivo es la distribución posterior.

**Ejemplo: Función de verosimilitud Bernoulli**

Retomemos el ejemplo del experimento Bernoulli, iniciamos con una función de 
distribución que describa nuestro conocimiento inicial $p(\theta)$ tal que 
podamos calcular la distribución objetivo $P(\theta) = \mathcal{L}(\theta)p(\theta)$ con facilidad. En este caso elegimos una densidad 
beta y podemos usar función dbeta de R:

```{r, eval}
# p(theta) con theta = 0.4, a = 2, b = 2
dbeta(0.4, 2, 2)
# Definimos la distribución inicial
prior <- function(a = 1, b = 1){
  function(theta) dbeta(theta, a, b)
}
```

También necesitamos especificar la función de verosimilitud, en nuestro caso 
tenemos repeticiones de un experimento Bernoulli por lo que: 
$$\mathcal{L}(\theta) = \theta^{z}(1-\theta)^{N-z}$$

y en R:

```{r}
# Verosimilitid binomial
likeBern <- function(z, N){
  function(theta){
    theta ^ z * (1 - theta) ^ (N - z)
  }
}

```

La distribución posterior $p(\theta|x)$ es, por la regla de Bayes,
proporcional a $p(x|\theta)p(\theta)$. Usamos este producto como la
distribución objetivo $P(\theta)$ en el algoritmo de Metrópolis.

```{r}
# posterior no normalizada
postRelProb <- function(theta){
  mi_like(theta) * mi_prior(theta)
}
```

Implementemos el algoritmo con una inicial Beta(1,1) (uniforme) y observaciones
$z = \sum{x_i}=11$ y $N = 14$, es decir lanzamos 14 volados de los cuales 11 
resultan en éxito (águila).

```{r, fig.height=4, fig.width=8}
# Datos observados
N = 14
z = 11

# Defino mi inicial y la verosimilitud
mi_prior <- prior() # inicial uniforme
mi_like <- likeBern(z, N) # verosimilitud de los datos observados
```

Proponemos una distribución normal $\mathcal{N}(0,0.1)$ como distribución propuesta:

```{r, fig.height=4, fig.width=8}
# para cada paso decidimos el movimiento de acuerdo a la siguiente función
caminaAleat <- function(theta){ # theta: valor actual
  salto_prop <- rnorm(1, 0, sd = 0.1) # salto propuesto
  theta_prop <- theta + salto_prop # theta propuesta
  if(theta_prop < 0 | theta_prop > 1){ # si el salto implica salir del dominio
    return(theta)
  }
  u <- runif(1) 
  p_move <-  min(postRelProb(theta_prop) / postRelProb(theta), 1) # prob mover
  if(p_move  > u){
    return(theta_prop) # aceptar valor propuesto
  }
  else{
    return(theta) # rechazar
  }
}

# Realizamos 6,000 pasos
set.seed(47405)

pasos <- 6000
camino <- numeric(pasos) # vector que guardará las simulaciones
camino[1] <- 0.1 # valor inicial

# Generamos la caminata aleatoria
for (j in 2:pasos){
  camino[j] <- caminaAleat(camino[j - 1])
}

caminata <- data.frame(pasos = 1:pasos, theta = camino)

# Graficamos los primeros 3,000 pasos
ggplot(caminata[1:3000, ], aes(x = pasos, y = theta)) +
  geom_point(size = 0.8) +
  geom_path(alpha = 0.5) +
  scale_y_continuous(expression(theta), limits = c(0, 1)) +
  scale_x_continuous("Tiempo") +
  geom_vline(xintercept = 600, color = "red", alpha = 0.5)
```

```{r, fig.height=4, fig.width= 6}
# Excluímos las primeras observaciones (etapa de calentamiento)
caminata_f <- filter(caminata, pasos > 600)
ggplot(caminata_f, aes(x = theta)) +
  geom_density(adjust = 2, aes(color = "posterior")) +
  labs(title = "Distribución posterior", 
       y = expression(p(theta)), 
       x = expression(theta)) + 
  stat_function(fun = mi_prior, aes(color = "inicial")) + # inicial
  xlim(0, 1)
```

Si la distribución objetivo es muy dispersa y la distribución propuesta muy 
estrecha, entonces se necesitarán muchos pasos para que la caminata aleatoria
cubra la distribución objetivo con una muestra representativa.

Por otra parte, si la distribución propuesta es muy dispersa podemos caer en 
rechazar demasiados valores propuestos. Imaginemos que $\theta_{actual}$ se
ubica en una zona de densidad alta, entonces cuando los valores propuestos 
están lejos del valor actual se ubicarán en zonas de menor densidad y 
$p(\theta_{propuesta})/p(\theta_{actual})$ tenderá a ser chico y el movimiento
propuesto será aceptado en pocas ocasiones.

**Ejercicio:** 

+ ¿Qué porcentaje de los valores propuestos fueron aceptados en el ejemplo anterior? 

+ Si cambia la desviación estándar de la distribución propuesta a
$\sigma = 0.01$ y $\sigma = 2$, ¿Cuál cambia el porcentaje de aceptación de
valores propuestos en cada uno de los dos casos? 

+ ¿De los 3 valores que usamos, qué desviación estándar crees 
que sea más conveniente?


De la muestra de valores de $p(\theta|x)$ obtenidos usando el algoritmo de
Metrópolis podemos estimar aspectos de la verdadera distribución $p(\theta|x)$.
Por ejemplo, para resumir la tendencia central es fácil calcular la media y 
la mediana.

```{r}
mean(caminata_f$theta)
sd(caminata_f$theta)
```

En el caso de predicción:

```{r}
sims_y <- rbinom(nrow(caminata_f), size = 1, prob = caminata_f$theta)
mean(sims_y) # p(y = 1 | x) probabilidad predictiva
sd(sims_y)
```



### Referencias

* [Understanding Computational Bayesian Statistics](https://www.amazon.com/Understanding-Computational-Bayesian-Statistics-William/dp/0470046090), William M. Bolstad.

* [Doing Bayesian Data Analysis](https://sites.google.com/site/doingbayesiandataanalysis/), John K. Kruschke.

* [Data Analysis Using Regression and Multilevel/Hierarchical models](http://www.stat.columbia.edu/~gelman/arm/), Andrew Gelman,
Jennifer Hill.


